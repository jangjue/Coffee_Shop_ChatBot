import json
import re
import time
import logging
from functools import lru_cache

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("utils")

def get_chatbot_response(client, model_name, messages, temperature=0):
    # Use list comprehension instead of for loop for better performance
    input_messages = [{"role": msg["role"], "content": msg["content"]} for msg in messages]
    
    # Estimate token count of input messages
    input_text = " ".join(msg["content"] for msg in messages)
    estimated_input_tokens = int(len(input_text.split()) * 1.3)  # rough token estimate
    
    # Calculate max tokens dynamically to avoid wasting tokens
    max_total_tokens = 8192
    max_response_tokens = max_total_tokens - estimated_input_tokens
    max_response_tokens = max(512, min(max_response_tokens, 2048))  # clamp to a reasonable range
    
    # Add retry mechanism with exponential backoff
    max_retries = 3
    retry_delay = 1.0
    
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="meta-llama/Meta-Llama-3-8B-Instruct",
                messages=input_messages,
                temperature=temperature,
                top_p=0.8,
                max_tokens=max_response_tokens,
                timeout=30,  # Set a reasonable timeout
            )
            return response.choices[0].message.content
        except Exception as e:
            if attempt < max_retries - 1:
                # Exponential backoff
                time.sleep(retry_delay)
                retry_delay *= 2
            else:
                # Return a safe default response on final failure
                return '{"decision": "allowed", "message": "Sorry, I encountered a temporary issue. Please try again.", "chain_of_thought": "Error in API call"}'


def get_embedding(embedding_client, model_name, text_input):
    output = embedding_client.embeddings.create(input=text_input, model=model_name)
    
    # Use list comprehension for better performance
    return [embedding_object.embedding for embedding_object in output.data]

# Pre-compile regex pattern for better performance
JSON_PATTERN = re.compile(r'({[\s\S]*})')

# Cache the JSON validation results to avoid repetitive processing
@lru_cache(maxsize=100)
def _validate_json_string(json_string):
    """Helper function to validate JSON string and cache results"""
    try:
        return json.loads(json_string), True
    except json.JSONDecodeError:
        return None, False

# Removed validate_response_text function
def double_check_json_output(client, model_name, json_string):
    """Check and correct JSON string to ensure validity"""
    # Fast path: if the string is already valid JSON, return it directly
    logger.info(f"double_check_json_output received: {json_string}") # Log input
    _, is_valid = _validate_json_string(json_string)
    if is_valid:
        parsed_json = json.loads(json_string)
        
        # Removed validation of the 'response' field content
        logger.info("double_check_json_output: Input is valid JSON.") # Log valid case
        return json.dumps(parsed_json)
    
    # If string is empty or None, return a default valid JSON structure
    if not json_string or json_string.strip() == "":
        logger.warning("double_check_json_output: Input string is empty. Returning default.") # Log empty case
        return '{"decision": "allowed", "message": "", "chain_of_thought": "Empty input"}'
    
    # Simple format fix attempt before calling API
    # Look for common JSON formatting issues and fix them
    simplified_json = json_string.strip()
    if not (simplified_json.startswith('{') and simplified_json.endswith('}')):
        # Try to extract JSON using regex
        json_match = JSON_PATTERN.search(simplified_json)
        if json_match:
            extracted_json = json_match.group(0)
            _, is_valid = _validate_json_string(extracted_json)
            if is_valid:
                # Removed validation of the 'response' field content in extracted JSON
                logger.info("double_check_json_output: Extracted valid JSON with regex.") # Log regex success
                parsed_json = json.loads(extracted_json)
                return json.dumps(parsed_json)
    
    # If local fixes failed, use LLM to repair the JSON
    logger.warning("double_check_json_output: Local fixes failed. Attempting LLM correction.") # Log LLM attempt
    prompt = f"""You will check this json string and correct any mistakes that will make it invalid. Then you will return the corrected json string. Nothing else.
    If the Json is correct just return it.

    Do NOT return a single letter outside of the json string.

    {json_string}
    """

    messages = [{"role": "user", "content": prompt}]
    logger.info(f"double_check_json_output: Sending correction prompt to LLM: {prompt}") # Log correction prompt
    
    # Use a lower temperature for more deterministic responses
    response = get_chatbot_response(client, model_name, messages, temperature=0.1)
    logger.info(f"double_check_json_output: Received correction response from LLM: {response}") # Log correction response
    
    # Extract JSON from the response
    json_match = JSON_PATTERN.search(response)
    
    if json_match:
        extracted_json = json_match.group(0)
        try:
            # Validate it's actually valid JSON by parsing it
            parsed_json = json.loads(extracted_json)
            
            # Removed validation of the 'response' field content after LLM correction
            logger.info("double_check_json_output: Successfully parsed JSON after LLM correction.") # Log LLM success
            return json.dumps(parsed_json)
        except json.JSONDecodeError:
            # If extraction failed, return a safe default
            logger.error("double_check_json_output: Failed to parse JSON even after LLM correction. Returning default.") # Log LLM parse fail
            return '{"decision": "allowed", "message": "", "chain_of_thought": "Failed to parse JSON"}'
    else:
        # If no JSON pattern found, return a safe default
        logger.error("double_check_json_output: No JSON found in LLM correction response. Returning default.") # Log LLM no JSON
        return '{"decision": "allowed", "message": "", "chain_of_thought": "No JSON found in response"}'